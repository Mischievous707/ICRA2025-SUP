# CTSG: Context and Topology based Multi-Modal Scene Graph for Visual Target Navigation
This repository is the official implementation of the paper: **CTSG: Context and Topology based Multi-Modal Scene Graph for Visual Target Navigation**. <!--[ğŸ”—](http://)-->

![å±å¹•æˆªå›¾ 2024-09-26 221652](https://github.com/user-attachments/assets/fbad25d8-99fc-4d05-a02e-0d933f31120c)

> https://github.com/user-attachments/assets/ff1d11a1-2961-4fb1-8d08-ae9c999f9cda

> https://github.com/user-attachments/assets/20362768-7054-4c69-925a-1913e1fb4b35
>
<!--
## Abstract
For visual target navigation tasks, a suitable environment representation plays a vital part in robot system to complete the navigation tasks. 3D Scene Graphs serve as sparse representations that excel at representing environments efficiently compare to dense semantic maps. Typical Scene Graphs are generally constructed based on multi-level semantic labels with hierarchical structures, which may lead to the loss of valuable spatial information, such as spatial topological relations, common sense understanding and visual context information. In this work, we propose **CTSG**, a Hierarchical 3D scene graph mapping approach for visual object navigation. Our graph adopts a dual-layer structure. Besides object layer, a novel conway (short for context information and way topology) layer is introduced, which consists of topological waypoints with rich multi-modal context information. We demonstrate the effectiveness of our method in novel visual target navigation tasks through simulation and real-world experiments across varied environments and instructions.
-->

## Video
https://github.com/user-attachments/assets/637966af-dfbb-458e-bb6a-15ddcbc1a3d8


## Approach

<img src="/img/pipeline.png" />

> todoï¼šæœ‰éœ€è¦å†è¡¥å……

CTSG is a dual-layer multi-modal scene graph. The conway layer contains navigable waypoints called conway nodes, which provide contextual information and multi-modal attributes like images and descriptions. 
The connections between conway nodes represent their navigability. 
The object layer includes object nodes, with attributes like individual descriptions and object images, and the connections between conway and object nodes reflect spatial relationships.

Based on our CSTG, we develop an efficient dual-level retrieval strategy for visual target navigation, which leverages VLMs to interpret semantic information and enrich representations. 
We conduct conway-level and object-level retrieval, coupled with a backtracking search to ensure robustness.

<!--
### Scene Graph Construction

<img src="/img/conwaygraph.png" />

> todoï¼šæ¨¡æ‹Ÿå™¨å›¾gifï¼ˆå›¾å·²é‡‡é›†å¤šç»„æ•°æ®ï¼ŒGLAQè¾ƒå·®ä½†èƒ½ä½“ç°å‡ºpose mapä½œç”¨ï¼‰

Building the topological structure involves a two-step process: identifying free regions and locating conway nodes.
The obstacle region map represents obstacles spatial distribution.
The pose region map is constructed by route points of fully exploring the environment.
The floor region map is obtained by projecting all floor-wise points into the BEV plane.

Based on above maps, we use the pose region map to complete the floor region map, then subtract obstacle elements from the obstacle map to obtain the navigatable floor free region.
Then, we select the largest area as final floor free region map for noise reduction.

To locate conway nodes, we utilize the Zhang-Suen thinning algorithm to extract the skeleton path of the free region, which is better suited for the localization of our sparse conway nodes. 
For conway nodes sparsity, a lower threshold $\tau$ is used to control the neighbouring nodes distance from the skeleton path.
Conway nodes are initially fully connected. 
We set an upper limit $\theta$ on the length of node-to-node distances and filter out edges passing through obstacle areas, ultimately deriving the sparse conway-layer.

<img src="/img/semantic.png" />

The description of conway nodes and object nodes are generated by VLMs.
To obtain the context information of conway nodes, we capture certain amount of perception views from each conway node.
The captured views are then stitched together to form a panorama, as visual contextual information of conway nodes from the surrounding environment. 

We employ a VLM, such as LLaVA-1.5, to generate detailed and comprehensive textual descriptions for each panorama, corresponding to components, locations and attributes.
These textual descriptions of conway nodes, together with the nodes' surrounding panoramas, form the multi-modal attributes of the conway nodes.

### Visual Target Navigation with Scene Graph

> **[M]**æ•´ä½“æ¶æ„å›¾+retrievalæ•´ä½“æ€è·¯ä¼˜åŠ¿æè¿°

#### Textualization of Visual Query

> todo: å›¾/case

Visual target navigation utilizes a single image as query.
We first use GPT-4o to obtain textual descriptions of query images, enriching their semantic information.
The target image, along with its contextual description, serves as a query unit for retrieval.

Then a distinguished prompting strategy is adopted based on the hierarchical characteristics of the nodes.
At the Conway level, our focus is on the contextual and positional information of the environment.
At the object level, we emphasize the detailed characteristics of individual objects.

#### Environment Database Construction
For each node in our graph, we use CLIP to encode its textual descriptions and associated visual image, serving as the semantic information stored in the vector databases.

#### Conway-level Retrieval

> todo: å›¾/case

We encode conway-level query unit using CLIP, compute its feature similarity with all conway-level nodes, and select the top-k candidate conway nodes.
Additionally, we retrieve surrounding conway nodes connected to these top-k candidates and calculate their similarity to the query unit, and select the top-3 secondary viewpoints for each.

We then input both the retrieved conway node information and the query unit into GPT-4o, leveraging its advanced analytical capabilities to select candidate conway node with the highest similarity as the optimal result.
The remaining candidate conway nodes are retained for potential backtracking in case of retrieval failure.

#### Object-level Retrieval

> todo: å›¾/case

Once the highest-possibility conway node is determined, we retrieve the object nodes associated with the chosen conway node.
The image and detailed object descriptions are used as the object-level query unit for retrieval. 
Similar to conway-level retrieval, the similarity between the CLIP features of the query unit and features of the object nodes is computed for selecting the top-3 most similar object nodes.

Then, we leverage GPT-4o model to analyze the multi-modal semantic information from the retrieved object nodes, referring to the query unit to re-rank the candidate object nodes and provide plausible candidates.

#### Backtracking Mechanism

> todo: å›¾/case

If the LLM cannot find any plausible candidate object, the retrieval process backtracks and select other conway-level alternatives until the query object is accurately identified.

#### Path Planning on Conway-layer

> todo: å›¾/case

Once a specific object node is located, the path planning module uses the corresponding conway node information to determine an optimal navigation path within the connected conway node graph using Dijkstra's algorithm.
This path is represented as a sequence of conway nodes coordinates.
Visual target navigation is accomplished by reaching the target in query image following the computed path.

-->

## Experiment & Analysis

[parameter_list](#section-heading)
<!--
### 1. Dataset

1. scene data

   HM3D

   ï¼ˆMatterport 3Dï¼‰

2. task data

   HM3D
-->
### [TODO: è‰³å¹³ä¿®æ”¹æ ‡é¢˜ï¼ŒåŠ æ€»èµ·å¥ä»‹ç»è¯„æµ‹äº†ä»€ä¹ˆ+ä¸ºä»€ä¹ˆè¯„æµ‹ï¼ˆæˆ‘ä»¬çš„ä¼˜åŠ¿ï¼‰+å®éªŒç»“æœ+åˆ†æï¼ˆåŠ ä¸Šconceptgraphï¼‰]1. Scene Graph 
ï¼Ÿå¦‚ä½•åˆç†å¼•å…¥conceptgraphï¼šå¸¸è§æ„å»ºæ–¹æ³•

   #### Graph Location
   
   ##### HM3D
   | Scenes | GON/SON | Accuracy(%) | Position Error(m) |
   |:--------:|:---------:|:-------------:|:-------------------:|
   |BAbdmeyTvMZ | 63/370 | 57.14 | 0.2682 |
   |Dd4bFSTQ8gi | 344 / 392 | 53.49 |  0.2319 |
   |QaLdnwvtxbs& | 238 / 238| 42.72 | 0.2964 |
   |svBbv1Pavdk | 286 / 372 | 47.20 | 0.4720 |
   | mean        |           | 50.14 | 0.3171|
   
   #### Matterport3D
   | Scenes | GON/SON | Accuracy(%) | Position Error(m) |
   |:--------:|:---------:|:-------------:|:-------------------:|
   |2azQ1b91cZZ | 695 / 862 | 44.75 | 0.3096 |
   |EU6Fwq7SyZv | 295/438 | 52.63 | 0.3228 |
   |oLBMNvg9in8 | 483/770 | 52.59 | 0.2554 |
   |TbHJrupSAjP | 421/641 | 60.57 | 0.2833 |
   |x8F5xyUWy9e | 93/276 | 41.92 | 0.2725 |
   |Z6MFQCViBuw | 152/562 | 58.55 | 0.3184 | 
   | mean        |           | 52.91 | 0.2956|
    
   
The accuracy of the constructed scene graph is evaluated on the Matterport3D and HM3D datasets. GON represents the number of predicted objects in the scene graph, SON represents the number of objects in the real scene, and Accuracy indicates the percentage of correctly predicted objects in the scene graph. The evaluation details are as follows: the object categories and object coordinates in the scene graph are matched with the ground truth categories and coordinates. An object prediction in the scene graph is considered correct only if the category labels match and the distance between the object coordinates in the scene graph and the ground truth coordinates is less than a threshold (1 meter). Position Error represents the average distance error between the correctly predicted objects in the scene graph and the ground truth objects.
   
To further demonstrate the accuracy of our proposed CSTG in constructing scene graphs, we include a comparative experiment with ConceptGraph. The experimental results are presented in the table.



obj location acc: å¯¹æ¯”ConceptGraphï¼Œhovsg?åœ¨gtç¯å¢ƒä¸‹è¯„æµ‹ 

1. gtä»habitat-simå¦‚ä½•è·å–
2. å¦‚ä½•å¯¹æ¯”ï¼ˆå’Œcompetetiveçš„xxæ–¹æ³•å¯¹æ¯”ï¼‰
3. ç»“æœï¼š

| dataset | scene | ConceptGraph | ours |
| ----------- | ----- | ------------ | ---- |
|      _       |       |              |      |
|       _      |       |              |      |

åˆ†æï¼š

æè¿°ï¼šä¸ºä»€ä¹ˆåšï¼Œæ€ä¹ˆåšï¼Œç»“æœå¦‚ä½•ï¼Œä»£è¡¨xx



### 2. Visual Target Navigation

 <!--
   conwayæ•´ä½“æ–¹æ³•ï¼ˆåŒ…æ‹¬æ„å›¾ã€æ£€ç´¢ï¼‰æœ‰æ•ˆæ€§

   â€‹	1. with/withoutï¼šåœ¨åŸæœ¬çš„åŸºç¡€ä¸Šå»æ‰conwayæ£€ç´¢=åªå¯¹æ‰€æœ‰objectåˆ—è¡¨åšclip+vlm

   > 2. room clusterï¼šåœ¨åŸæœ¬çš„åŸºç¡€ä¸ŠæŠŠconwayæ¢æˆroomï¼ˆèšç±»å¾—åˆ°ï¼Œæœ‰å›¾æ–‡ï¼‰ï¼Œå…¶ä»–ä¸å˜
   -->

<div style="text-align: center;">

|    Scene     | IEVE       |          | Object Only |     | Ours  |          |
|:------------:|:----------:|:--------:|:-----------:|:---:|:-----:|:--------:|
|              | sr(%)      | spl      | sr(%)       | spl | sr(%) | spl      |
| Dd4bFSTQ8gi  | 33.6       | 0.0753   | 24.00       | 0.7360    | 41.33 |  0.6029  |
| QaLdnwvtxbs  | 66.4       |  0.2681  | 48.15       |   0.5256  | 44.44 |  0.7471  |
| VBzV5z6i1WS  | 44.8       | 0.1517   | 34.38       |   0.6562  | 48.96 |  0.6090  |
| ziup5kvtCCR  | 45.6       |  0.1332  | 43.59       |  0.5086   | 41.03 |  0.5979  |
| Nfvxx8J5NCo  | 35.2       |  0.1236  | 25.64       |  0.6327   | 47.37 |  0.6264  |
| svBbv1Pavdk  | 57.6       |  0.2126  | 17.54       |  0.6283   | 42.42 |  0.6214  |
| BAbdmeyTvMZ  | 60.8       |  0.3492  | 39.40       |   0.7001  | 42.70 | 0.6452   |
| mv2HUxq3B53  | 37.6       |  0.1208  | 40.00       |  0.6248   | 22.67 |  0.6423  |
| Mean         | 47.9       |  0.1823  | 34.09       |    0.6265  | 40.19 |  0.6365  |

</div>

Initially, we built a object-baseline model that directly searches for the target instance from the list of all detected object instances in the scene. After identifying the target, a navigation graph is used to plan the path and complete the task. However, this flat retrieval approach, which focuses only on the object itself and ignores the surrounding context, limits accurate recognition of the target object to longer distances, reducing navigation success rates.

We also compared our navigation results with the state-of-the-art IEVE method on our evaluation dataset. While IEVE slightly outperforms our method in navigation performance, it relies on a training process to optimize model parameters. In contrast, our method uses a training-free approach, aiming to evaluate its generalization ability by directly applying the model to new environments, offering a more flexible solution for real-world applications.

### 3. Retrieval Strategy Evaluation

<!--
We designed a model to eliminate uncertainties inherent in Large Visual and Language Models (LVLMs) to precisely evaluate the retrieval performance of our scene graph, basing all node selections just on CLIP similarity decisions. During the model's construction, we removed all LVLM-related modules.
-->

We removed all Large Visual and Language Models (LVLMs) related modules during model's construction for elimination inherented uncertainties in order to precisely evaluate the retrieval performance of our scene graph, basing all node selections just on CLIP similarity decisions. 

> todoï¼šå’Œæˆ‘ä»¬æ–¹æ³•æ•ˆæœä¸Šçš„æ¯”è¾ƒ

   #### 1. The Effectiveness of Top-k Selection on Conway Layer Retrieval
   <!--
   ä¸ºäº†æ‰¾åˆ°æœ€å¥½çš„K
   å»æ‰VLMä¸ç¡®å®šå› ç´ ï¼Œæ¢è®¨clipç»“æœçš„precision
   é€‰å‡ºæœ€åˆé€‚çš„K
   æˆ–è€…
   åŠ ä¸Švlmï¼Œè¯„æµ‹ä¸¤ä¸ªå†…å®¹ï¼š1. clipé€‰æ‹©çš„ç»“æœï¼Œ2. VLMå¯¹ä¸åŒKçš„è¡¨ç° 
   -->
<div style="text-align: center;">

| K-Selection  |  Top 1  |  Top 3  |  Top 5  |  Top 7  |
|:------------:|:-------:|:-------:|:-------:|:-------:|
| Conway Acc   |  49.09  |  47.69  |  46.93  |  44.64  |
| Object Acc   |  27.03  |  29.00  |  31.21  |  30.01  |

</div>	
   <!-- å®éªŒè®¾ç½®ï¼šæœ¬ç ”ç©¶è®¾è®¡äº†ä¸€ç§æ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ’é™¤äº†å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆLVLMï¼‰ä¸­çš„ä¸ç¡®å®šæ€§å› ç´ ï¼Œæ‰€æœ‰èŠ‚ç‚¹çš„é€‰æ‹©å‡åŸºäºCLIPç›¸ä¼¼åº¦è¿›è¡Œå†³ç­–ã€‚åœ¨æ„å»ºæ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬ç§»é™¤äº†ä¸æ‰€æœ‰LVAMç›¸å…³çš„æ¨¡å—ã€‚åœ¨ConwayèŠ‚ç‚¹çš„é€‰æ‹©ä¸Šï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸€ç§ç­–ç•¥ï¼šå°†æ‰€æœ‰å€™é€‰ConwayèŠ‚ç‚¹ä¸‹æŒ‚è½½çš„ç‰©ä½“èŠ‚ç‚¹çš„CLIPç‰¹å¾ä¸ç›®æ ‡ç‰©ä½“å›¾ç‰‡çš„CLIPç‰¹å¾è¿›è¡Œç›¸ä¼¼åº¦åŒ¹é…ï¼Œé€‰æ‹©æœ€ç›¸ä¼¼çš„ç‰©ä½“èŠ‚ç‚¹ï¼Œå°†å…¶è¿æ¥çš„ConwayèŠ‚ç‚¹ä½œä¸ºå¯¼èˆªç‚¹ä»¥è§„åˆ’è·¯å¾„ã€‚å®éªŒç»“æœå¦‚ä¸Šè¡¨æ‰€ç¤ºã€‚
   åˆ†æç»“æœè¡¨æ˜ï¼Œå½“kå€¼è®¾å®šä¸º3æˆ–5æ—¶ï¼Œå¯ä»¥è¾ƒå¥½åœ°å¹³è¡¡Conwayå’Œç‰©ä½“çš„æ£€ç´¢æ­£ç¡®ç‡ã€‚ç„¶è€Œï¼Œå½“kå€¼å¢è‡³5æ—¶ï¼Œè™½ç„¶ç‰©ä½“å‡†ç¡®ç‡çš„æå‡æœ‰é™ï¼Œä½†å¯¼èˆªå‡†ç¡®ç‡å´æœ‰æ‰€ä¸‹é™ã€‚æ­¤å¤–ï¼Œè€ƒè™‘åˆ°æˆ‘ä»¬çš„æ–¹æ³•ä¼šè°ƒç”¨LVLMï¼Œkå€¼çš„å¢å¤§ä¼šå¯¼è‡´è¾“å…¥åˆ°è§†è§‰è¯­è¨€æ¨¡å‹çš„tokenæ•°é‡å¢åŠ 67%ã€‚ç»¼åˆè€ƒè™‘ï¼Œæˆ‘ä»¬é€‰æ‹©kå€¼ä¸º3ä½œä¸ºæœ€ä¼˜è§£-->
<!--
> For the selection of Conway nodes, we adopted a strategy where we matched the CLIP features of all candidate Conway nodes' attached object nodes with the CLIP features of the target object image to find the most similar object node. The Conway node connected to this object node was then used as the navigation point to plan the path. The experimental results are shown in the table above.
>
> Analysis of the results indicates that setting the k value to 3 or 5 can achieve a good balance between the retrieval accuracy of Conway and object nodes. However, when the k value is increased to 5, although the improvement in object accuracy is limited, the navigation accuracy decreases. Moreover, considering that our method invokes LVLMs, an increase in the k value leads to a 67% increase in the number of tokens input to the visual language model. Taking all factors into account, we have chosen a k value of 3 as the optimal solution.

For Conway nodes selection, we used a strategy where the CLIP features of candidate Conway nodes' attached object nodes were matched with the CLIP features of the target object image to identify the most similar object node. The Conway node linked to this object node was then chosen as the navigation point for path planning. The experimental results are shown in the table above.

-->

Result analysis indicates that setting the k-value to 3 or 5 achieves a good balance between retrieval accuracy for Conway and object nodes. However, when the k-value set up to 5, while the object accuracy sees limited improvement, navigation accuracy also declines. 
Additionally, since our method invokes LVLMs, increasing the k-value leads to a 67% rise in the number of tokens input to the visual language model. 

Considering all factors, we have chosen a k-value of 3 as the optimal solution.


   #### 2. The Effectiveness of Multimodal Information in Scene Graphs
   
   | Method        | Modal       | Top1  | Top3  | Top5  |
   |:-------------:|:-----------:|:-----:|:-----:|:-----:|
   |               | Multi-Modal | 7.53  | 14.93 | 19.07 |
   | Room-Object   | Image Only  | 4.26  | 14.28 | 17.29 |
   |               | Text Only   | 4.35  | 13    | 15.38 |
   |               | Multi-Modal |       |       |       |
   | Conway-Object | Image Only  |       |       |       |
   |               | Text Only   |       |       |       |

<!-- å•æ¨¡æ€å¤šæ¨¡æ€ 1. å»æ‰vlmçš„æˆ‘ä»¬çš„æ–¹æ³•ï¼Œçº¯è¯„ä¼°graphï¼š clip conway top 3 -> expand to 9 -> clip obj top 1 2.å»æ‰æ–‡æœ¬æ¨¡æ€ï¼šclip conway top 3 -> expand to 9 -> clip objï¼ˆonly imageï¼‰ top 1 3. å»æ‰å›¾åƒæ¨¡æ€ï¼š clip conway top 3 -> expand to 9 -> clip objï¼ˆonly textï¼‰ top 1 5. conway æ¢æˆgt room clip room label top 3 -> clip obj top 1 6. å»æ‰å›¾åƒæ¨¡æ€ï¼ˆè®ºæ–‡baselineï¼‰clip room label top 3 -> clip objï¼ˆonly textï¼‰ top 1 -->
<!--å®éªŒè®¾ç½®ï¼šæˆ‘ä»¬å¯¹room-objectå’Œconway-objectä¸¤ç§ä¸åŒçš„å½¢å¼çš„graph, åœ¨ç›¸åŒå®éªŒè®¾ç½®ä¸‹åšå¯¹æ¯”ï¼Œå®éªŒç»“æœå¦‚ä¸Šè¡¨ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°ï¼Œä¸è®ºæ˜¯room-objectå½¢å¼è¿˜æ˜¯conway-objectå½¢å¼çš„åœºæ™¯graphï¼Œå¢åŠ èŠ‚ç‚¹ä¸Šçš„æ¨¡æ€ä¿¡æ¯å¯ä»¥æå‡ç‰©ä½“æ£€ç´¢çš„å‡†ç¡®ç‡ã€‚å…¶æ¬¡ï¼Œåœ¨ç›¸åŒå®éªŒé…ç½®ä¸‹ï¼Œconway-objectå½¢å¼çš„graphæ¯”room-objectå½¢å¼çš„graphæ£€ç´¢çš„ç»“æœè¦å¥½çš„å¤šã€‚è¿™ä¸ªå®éªŒå……åˆ†è¯æ˜äº†æˆ‘ä»¬æå‡ºçš„å¤šæ¨¡æ€conway-objåœºæ™¯å›¾çš„ä¼˜åŠ¿ã€‚-->
<!--
> We compared the performance of two different graph structuresâ€”the room-object and conway-object graphsâ€”under uniform experimental conditions. The results of the experiment are detailed in the aforementioned table. Preliminary observations indicate that regardless of whether the room-object or conway-object graph form is utilized, augmenting the nodes with multimodal information significantly enhances the accuracy of object retrieval.
>
> Further analysis revealed that under identical experimental configurations, the conway-object graph outperformed the room-object graph in retrieval performance. This finding fully confirms the significant advantages of the multimodal conway-object scene graph we proposed in retrieval tasks.
-->

We compared the performance of two different graph structuresâ€”the room-object graph and the Conway-object graphâ€”under consistent experimental conditions. 

The results are detailed in the table above.

>todo: å®éªŒç»“æœåˆ†æ
         
<!--### 4. å¤šä»»åŠ¡task-->

      


## å‚æ•°è¡¨<a name="section-heading"></a>

| Subsystem | Hyperparameters | Value |
| --------- | --------------- | ----- |
|     _      |                |       |
|    _       |                 |       |
|       _    |                 |       |

## Appendix
### Instance Segmentation Model Selection

We evaluate our instance segmentation on the Replica dataset, following the same evaluation protocol as described in Conceptgraph. The segmentation performance of current models is compared using two metrics: Mean Accuracy (mAcc) and Frequency-weighted Intersection over Union (F-mIoU).

| Models      | Type      | mAcc  | F-mIOU |
|:-----------:|:---------:|:-----:|:------:|
| CLIPSeg     |           | 28.21 | 39.84  |
| LSeg        | Fine Tune | 33.39 | 51.54  |
| OpenSeg     |           | 41.19 | 53.74  |
| MaskCLIP    |           | 4.53  | 0.94   |
| Mask2former | Zero Shot | 10.42 | 13.11  |
| RAM + SAM   |           | 34.19 | 37.27  |
| RAM + SAM2  |           | 35.80 | 38.35  |

> todo:é‡å†™åˆ†æ

> we report various semantic segmentation results on Replica dataset. The combination of the RAM and SAM series demonstrates a significant advantage over MaskCLIP and Mask2former. Although its performance metrics are slightly lower than those of fine-tuned models, the open-vocabulary segmentation model demonstrates a clear advantage in scene transferability. There is no significant performance difference between the SAM and SAM2 models, but SAM is more widely-used and adaptable to other models. Therefore, we selected the RAM+SAM combination for object segmentation in scenes.





